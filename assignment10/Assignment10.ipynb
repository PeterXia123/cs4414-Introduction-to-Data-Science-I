{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import pydotplus\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: Nested Spheres\n",
    "\n",
    "Simulation is an incredibly useful tool in data science.  We can use simulation to evaluate how algorithms perform against ground truth, and how algorithms compare to one another.\n",
    "\n",
    "In this assignment, you will be implementing and extending the nested spheres simulation study found in *Elements of Statistical Learning* page 339.  \n",
    "\n",
    "# Nested Spheres\n",
    "\n",
    "Consider a dataset which contains 10 features $X_1 \\,, X_2 \\,, \\cdots \\,, X_{10}$.  The features are standard independent Gaussian random variables.  That is to say\n",
    "\n",
    "$$ X_j \\sim \\operatorname{Normal}(0,1) \\quad \\forall j = 1 \\dots 10$$\n",
    "\n",
    "We are going to use these features to study a classification problem.  You will have to create the target variable, $Y$ by computing the following rule:\n",
    "\n",
    "$$ Y = \\begin{cases}  1 \\quad \\mbox{ if } \\sum_{j=1}^{10} X^2_j>9.34 \\\\ -1 \\quad  \\mbox{else} \\end{cases}$$\n",
    "\n",
    "# The Simulation Study\n",
    "\n",
    "Follow these steps to complete the assignment. \n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. Generate a training data set of 2000 observations according to the description above.  Label each of these training examples according to the rule above.\n",
    "\n",
    "2. Train a bagged estimator, a random forrest with `max_features=1`, a random forest with `max_features=3`, and an additional model of your choice (you can increase max features again, or you can explore a boosted estimator).  Use 500 trees in your random forests and bagged estimator.\n",
    "\n",
    "3.  Generate a testing data set of 10,000 observations according to the description above.  Label each of these training examples according to the rule above.\n",
    "\n",
    "4.  Use each model to predict on the testing data.  Record the testing error rate (that is 1 - accuracy).\n",
    "\n",
    "\n",
    "Repeat these steps 50 times.  Plot the error rates as a box plot by model to complete the assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    \n",
    "    '''\n",
    "    This function creates the data according to the rule above.\n",
    "    \n",
    "    Inputs:\n",
    "    N - integer.  The number of samples to draw.\n",
    "    '''\n",
    "    # Some logic to prevent some errors\n",
    "\n",
    "    # Generate the features to learn from.\n",
    "    # Features are iid standard gaussian, so draw from a multivariable standar normal in which the \n",
    "    # covariance matrix is the identity\n",
    "    \n",
    "    X = np.random.normal(size = (N, 10))\n",
    "    # Calculate the sum to determine if y=0 or y=1\n",
    "    \n",
    "    radius_squared  = np.linalg.norm(X, 2, axis = 1)**2\n",
    "    \n",
    "    y = radius_squared>9.34\n",
    "    \n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a252a9490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_simulations = 50\n",
    "err= np.zeros(number_of_simulations)\n",
    "err_rf1= np.zeros(number_of_simulations)\n",
    "err_rf2= np.zeros(number_of_simulations)\n",
    "err_rf3= np.zeros(number_of_simulations)\n",
    "err_boost = np.zeros(number_of_simulations)\n",
    "\n",
    "for i in range(number_of_simulations):\n",
    "    #step1 \n",
    "    Xtrain, ytrain = generate_data(2000)\n",
    "    bag = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500)\n",
    "    rf1 = RandomForestClassifier(n_estimators=500,max_features=1)\n",
    "    rf3 = RandomForestClassifier(n_estimators=500,max_features=3)\n",
    "    boost = XGBClassifier(n_estimators=500)\n",
    "\n",
    "# fit model no training data\n",
    "\n",
    "    #step2 fit the model\n",
    "    bag.fit(Xtrain, ytrain)\n",
    "    rf1.fit(Xtrain, ytrain)\n",
    "    rf3.fit(Xtrain, ytrain)\n",
    "    boost.fit(Xtrain, ytrain)\n",
    "# fit model no training data\n",
    "\n",
    "    #step3 generate test data \n",
    "    Xtest,ytest = generate_data(10000)\n",
    "    #step4 \n",
    "    ypred = bag.predict(Xtest)\n",
    "    ypred_rf1 = rf1.predict(Xtest)\n",
    "    ypred_rf3 = rf3.predict(Xtest)\n",
    "    ypred_boost = boost.predict(Xtest)\n",
    "    #calculate the errror\n",
    "    err[i] = 1 - accuracy_score(ytest,ypred)\n",
    "    err_rf1[i] = 1 - accuracy_score(ytest,ypred_rf1)\n",
    "    err_rf3[i] = 1- accuracy_score(ytest, ypred_rf3)\n",
    "    err_boost[i] = 1 - accuracy_score(ytest,ypred_boost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "estimators = {'bag': err, 'rf1': err_rf1,'rf3':err_rf3,'boost':err_boost}\n",
    "\n",
    "df = pd.DataFrame(estimators)\n",
    "\n",
    "\n",
    "df.boxplot()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the barplot we can see the bagging has the highest error rate. followed by random forest with 3 max_features. and the random forest with 1 max_feature has the second lowest error rate. i think the main reason why rf1 is better than rf3 is the features space is only 10. finallly, the boost method has the lowest error rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
